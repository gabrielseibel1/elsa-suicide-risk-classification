```{r setup}
knitr::opts_knit$set(root.dir = '../..') # this should be knitted from the repo root directory
```


```{r}

source("scripts/setup.R")
```


```{r}
fold_data <- readRDS("reports/results/models_and_evals/glmnet:::MODE_PRESENCE:::LABEL_H:::SYMP_TRUE:::CV_TRUE/data_info.rds")
fold_data$splits
```


```{r}

all_tests <- tibble()

for (i in seq_len(nrow(fold_data))) {

  tests_glmnet <- get_test_from_saved_model(METHOD_GLMNET, fold_data$splits[[i]], i, 403) %>%
    transmute(
      glmnet = POSITIVE,
      RISK = map_lgl(obs, ~. == 'POSITIVE')
    )

  tests_rf <- get_test_from_saved_model(METHOD_RF, fold_data$splits[[i]], i, 403) %>%
    transmute(rf = POSITIVE)

  tests_nb <- get_test_from_saved_model(METHOD_NAIVE_BAYES, fold_data$splits[[i]], i, 403) %>%
    transmute(nb = POSITIVE)

  tests_ensemble <- get_test_from_saved_model(METHOD_ENSEMBLE, fold_data$splits[[i]], i, 403) %>%
    transmute(ensemble = POSITIVE)

  tests_fold <- tests_glmnet %>%
    bind_cols(tests_rf) %>%
    bind_cols(tests_nb) %>%
    bind_cols(tests_ensemble)

  all_tests <- bind_rows(all_tests, tests_fold %>% add_column(fold = i))
}

all_tests %<>% select(RISK, everything())
as_tibble(all_tests)
```


Build predictions of a series model ("sm"). Make it a function since it will be used for different thresholds.

```{r}

get_sm_tests <- function(tests, nb_threshold, rf_threshold, fold_nr) tests %>%
  filter(fold == fold_nr) %>%
  mutate(nb_pred = nb > nb_threshold, rf_pred = rf > rf_threshold) %>%
  mutate(sm_pred = nb_pred & rf_pred) %>%
  transmute(obs = as.factor(RISK), pred = as.factor(sm_pred))

get_sm_tests(all_tests, .5, .5, 1)
```



```{r}

get_metrics_for_thresholds <- function(threshold_nb, threshold_rf, fold_nr) {
  tests <- get_sm_tests(all_tests, threshold_nb, threshold_rf, fold_nr)
  metrics <- caret::confusionMatrix(tests$pred, tests$obs, "TRUE", mode = "everything")$byClass

  tibble(
    fold = fold_nr,
    thresh_nb = threshold_nb,
    thresh_rf = threshold_rf,
    sens = metrics["Sensitivity"],
    spec = metrics["Specificity"],
    ppv = metrics["Pos Pred Value"],
    npv = metrics["Neg Pred Value"],
    prec = metrics["Precision"],
    rec = metrics["Recall"]
  ) %>%
    mutate(
      f2 = 5 * prec * rec / (5 * prec + rec)
    )

}

get_metrics_for_thresholds(.5, .5, 1)
```


For a set of thresholds, for all folds, get performance estimate metrics

```{r}

ts <- seq(0.05, 0.95, by = 0.05)
(thresholds_metrics <- expand.grid(folds = 1:30, thresh_nb = ts, thresh_rf = ts))

```


```{r}

folds <- thresholds_metrics$folds
nbs <- thresholds_metrics$thresh_nb
rfs <- thresholds_metrics$thresh_rf
all_metrics <- pmap_dfr(list(nbs, rfs, folds), get_metrics_for_thresholds)

head(all_metrics)
```

Take mean and std dev across folds

```{r}
sum_metrics <- all_metrics %>%
  replace_na(list(sens = 0, spec = 0, ppv = 0, npv = 0, prec = 0, rec = 0, f2 = 0)) %>%
  group_by(thresh_nb, thresh_rf) %>%
  summarise(
    sens_mean = mean(sens),
    sens_sd = sd(sens),

    spec_mean = mean(spec),
    spec_sd = sd(spec),

    ppv_mean = mean(ppv),
    ppv_sd = sd(ppv),

    npv_mean = mean(npv),
    npv_sd = sd(npv),

    prec_mean = mean(prec),
    prec_sd = sd(prec),

    rec_mean = mean(rec),
    rec_sd = sd(rec),

    f2_mean = mean(f2),
    f2_sd = sd(f2)
  )

sum_metrics
```


```{r}

sum_metrics_pruned <- sum_metrics %>% filter(sens_mean > 0.6 & spec_mean > 0.5)

sum_metrics_pruned %>% head()
```


```{r}
sum_metrics_pruned %>% dplyr::arrange(desc(sens_mean)) %>% head()
```

```{r}
data <- sum_metrics_pruned %>% dplyr::arrange(desc(sens_mean))

ggplot2::ggplot(data, aes(x = spec_mean, y = sens_mean)) +
        ggplot2::geom_point()
```


































