# Models tests thresholds

Setup

```{r setup}
knitr::opts_knit$set(root.dir = '../..') # this should be knitted from the repo root directory
```


```{r}

source("scripts/setup.R")
```

Load tests

```{r}

# TODO: enumerate folds
all_tests <- readRDS("notebooks/tests_probs/tests.rds")
all_tests
```

Build predictions of a series model ("sm"). Make it a function since it will be used for different thresholds.

```{r}

get_sm_tests <- function(tests, nb_threshold, rf_threshold) tests %>%
  mutate(nb_pred = nb > nb_threshold, rf_pred = rf > rf_threshold) %>%
  mutate(sm_pred = nb_pred & rf_pred) %>%
  transmute(obs = as.factor(RISK), pred = as.factor(sm_pred))

```

Test it with .5

```{r}

get_sm_tests(all_tests, .5, .5)
```


```{r}

get_metrics_for_thresholds <- function(threshold_nb, threshold_rf) {
  tests <- get_sm_tests(all_tests, threshold_nb, threshold_rf)
  metrics <- caret::confusionMatrix(tests$pred, tests$obs, "TRUE", mode = "everything")$byClass

  tibble(
    thresh_nb = threshold_nb,
    thresh_rf = threshold_rf,
    sens = metrics["Sensitivity"],
    spec = metrics["Specificity"],
    ppv = metrics["Pos Pred Value"],
    npv = metrics["Neg Pred Value"],
    prec = metrics["Precision"],
    rec = metrics["Recall"]
  ) %>%
    mutate(
      f2 = 5 * prec * rec / (5 * prec + rec)
    )

}

get_metrics_for_thresholds(.5, .5)
```

For a set of thresholds, get performance estimate metrics

```{r}

ts <- seq(0.05, 0.95, by = 0.05)
( thresholds_metrics <- expand.grid(thresh_nb = ts, thresh_rf = ts) )

```


```{r}

nbs <- thresholds_metrics$thresh_nb
rfs <- thresholds_metrics$thresh_rf
map2_dfr(nbs, rfs, get_metrics_for_thresholds)
```























